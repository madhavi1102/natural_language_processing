{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd28c30",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Word Embeddings are the vector representation of the words where words with similar meaning or context are closer in the vector space and they have similar representations.\n",
    "\n",
    "Usage: \n",
    " sentiment analysis, document clustering, question answering, paraphrase detection \n",
    " \n",
    " 1. Word2vec\n",
    " 2. Glove\n",
    " 3. BERT\n",
    " 4. USE\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0dd7d7",
   "metadata": {},
   "source": [
    " ## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa334ef",
   "metadata": {},
   "source": [
    "1. Word2vec is a two-layer feedforward neural network (shallow neural network).\n",
    "2. There are two architectures in Word2Vec:    ? independtly or in combination\n",
    "    1. Continous Bag of Words (CBOW)\n",
    "    2. Skip-Gram\n",
    "    \n",
    "Word2Vec have input layer of vocabulary size, followed by hidden layer of vector size(300) and then output layer of vocabulary size. we either train the word on its context (skip-gram) or train the context on the word (continuous bag of words) using a 1-hidden layer neural network.\n",
    "\n",
    "Con:\n",
    "\n",
    "Word2vec model only considers the local (nearby) context words of the target word for it's vector representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0165f62",
   "metadata": {},
   "source": [
    "### Skipgram \n",
    "\n",
    "1. The network is trained using word pairs generated from the corpus.\n",
    "\n",
    "2. For each word in a sentence, we generate word pairs by looking at a \n",
    "fixed number of words before and after the current word (or the input word). \n",
    "This fixed number of words is also known as the window size. it is 2 or 3 or 4 etc\n",
    "\n",
    "3. Window size of 3 means that we look at 3 words before and 3 words after the current word , \n",
    "these are context words wt-3,wt-2,wt-1,wt+1,wt+2,wt+3.\n",
    "\n",
    "4. word pairs generated are (wt,wt-3), (wt,wt-2), (wt,wt-1), (wt,wt+1), (wt,wt+2), (wt+3). \n",
    "These word pairs generated are used to train the Neural Network.That is wt is given as input to the model and output is one of the \n",
    "context words.\n",
    "\n",
    "5. Assume that the vocabulary size of the corpus is 10,000. This means that there are 10,000 unique words in the corpus.\n",
    "    The input to the network is a one-hot encoded vector representing the input word. \n",
    "    The network has one hidden layer with 300 neurons. The output layer has 10,000 neurons, \n",
    "    one for each word, with a softmax activation. No activation is used in the hidden layer.\n",
    "    The presence of softmax means that the model will actually output probabilities for 10,000 words. \n",
    "    This probability is the probability of the word at that index being the nearbuy or a context word for the input/current word.\n",
    "    Intuitively, words that occur near the input word multiple times in the corpus will have a larger probability than others.\n",
    "\n",
    "6. The hidden layer is associated with a matrix of size (10,000 X 300), one 300 dimensional vector for each word in the corpus. \n",
    "    After backpropagation, the values of this matrix represent our word vectors.So the proposed task was fake because we never used the output layer.\n",
    "    We just wanted to learn representations of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab3d80",
   "metadata": {},
   "source": [
    "## Subsampling \n",
    "Certain  words which are present hugely but have less meaning to the sentence. Words like \"the\", \"and\", \"a\" etc. might occur in many context windows and hence be a part of many word-pairs. subsampling is introduced that deletes words with high frequency from the corpus. each word is assigned a probability of whether to keep it or drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165f64c",
   "metadata": {},
   "source": [
    "## Negative Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcab5b7",
   "metadata": {},
   "source": [
    "## CBOW\n",
    "\n",
    "continuous bag-of-words (CBOW) model predicts the input word from the context words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada80676",
   "metadata": {},
   "source": [
    "# GLoVe  (Globel Vectors)  (statistical approach)\n",
    "\n",
    "1. Glove is a count-based model which works upon generated word-word co-occurence matrix also called count matrix from the corpus. It does not use neural network\n",
    "\n",
    "\n",
    "2. Glove consider global context words to generate vector representation of a given word by creating word-word co-ooccurrence matrix.\n",
    "3. co- occurrence matrix contains words(vocabulary) in rows and columns. It has size V*V , where V is number of words in the vocabulary. \n",
    "\n",
    "4. co-occurrence matrix is a symmetric matrix\n",
    "5. context meanings neighbouring words.\n",
    "\n",
    "\n",
    "6. GLOVE learns by constructing a co-occurrence matrix (words X context) that basically count how frequently a word appears in a context. Since it's going to be a gigantic matrix, we factorize this matrix to achieve a lower-dimension representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e579c2",
   "metadata": {},
   "source": [
    "## Loading GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17861ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the Glove pretrained model\n",
    "def load_glove_vectors(tokenizer,max_features):\n",
    "    embeddings_index=dict()\n",
    "    embed_size=50\n",
    "\n",
    "    with open(\"glove.6B.50d.txt\", encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            word=values[0]\n",
    "            coefs=np.asarray(values[1:],dtype='float32')\n",
    "            embeddings_index[word]=coefs\n",
    "            \n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    print(\"Loaded %s word vectors\"%len(embeddings_index))\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51392962",
   "metadata": {},
   "source": [
    "## Loading Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filepath=\"./GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "def load_word2vec_embeddings(filepath, tokenizer, max_features, embedding_size):\n",
    "    model = KeyedVectors.load_word2vec_format(filepath, binary=True)\n",
    "\n",
    "    emb_mean, emb_std = model.wv.syn0.mean(), model.wv.syn0.std()\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb51b0",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/   ****\n",
    "\n",
    "https://www.mygreatlearning.com/blog/word-embedding/\n",
    "https://machinelearninginterview.com/topics/natural-language-processing/what-is-the-difference-between-word2vec-and-glove/\n",
    "\n",
    "https://deeplearning.lipingyang.org/wp-content/uploads/2017/12/How-is-GloVe-different-from-word2vec_-Quora.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
