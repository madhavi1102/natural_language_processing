{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis thru eyes\n",
    "Intially , text analysis is done mannually.we check in high level the variation in length of text,what characters are mostly used,what are unwanted symbols and words\n",
    "\n",
    "# Text Cleaning\n",
    "\n",
    "Natural language text contains various unwanted characters or words which donot add value to the problem statement and reduce the performance of model. we perform various cleaning steps in text preprocessing such as \n",
    "\n",
    "1. Removing punctuations,numeric digits,unwanted symbols\n",
    "2. Remving urls, hashtags , html tags etc\n",
    "3. Removing stopwords: stopwords are those words which occur oftenly in the set of texts (corpus),or words that are not significant to particular text, such as the, a,an,and,where,now,from,at,etc\n",
    "4. lowercasing : python is casesensitive. converting all texts into single case, because the same word with different cases are considered different.\n",
    "5. Stemming and Lemmatizating : convert the words to it's base(root) form. \n",
    "\n",
    "    5.1  Stemming does by truncating the suffixes,last set of characters. Developing -> Develop, cuddling ->cuddl,      leaves ->leav\n",
    "    \n",
    "    5.2  Lemmatizing converts to the root form of the words by specifying tag of the words such as verb,noun,adjectives. cuddling -> cuddle, leaves ->leaf\n",
    "    \n",
    "\n",
    "6. expanding abbrevations\n",
    "\n",
    "7. tokenization : splitting the text into words also called tokens\n",
    "\n",
    "\n",
    "\n",
    "### Packages in use in text preprocessing:\n",
    "\n",
    "\n",
    "1. nltk\n",
    "2. spacy\n",
    "3. gensim\n",
    "4. re (regular expression)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string \n",
    "\n",
    "# not showing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download predefined stop words from nltk.corpus package and can also update it add or remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mkumari9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "# stopwords for english\n",
    "stop_words=set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuations Removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "This  is my  test\n"
     ]
    }
   ],
   "source": [
    "# set of punctuation symbols\n",
    "punctuation_marks=string.punctuation\n",
    "print(punctuation_marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace : string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use string.replace method the fast way to remove the special characters\n",
    "# replace method checks for substring and do the extach match of the substring.\n",
    "print(\"This #$ is my  test!\".replace(\"#$\",\"\").replace(\"!\",\"\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text:  This is most # impi$% for me!.\n",
      "After preprocessing:  This is most  impi for me\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# using regex expression to remove the sepcial characters  except .\n",
    "rexp=re.compile(r'[!\\\"#$%&\\'()*+,-/:;<=>?@[\\]^_`{|}~]')      # regex expression   #? why dot mark is not coming in the result\n",
    "text1=\"This is most # impi$% for me!.\"\n",
    "print(\"The original text: \",text1)\n",
    "text1=rexp.sub(\"\",text1)\n",
    "print(\"After preprocessing: \",text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expression (regex)\n",
    "\n",
    "re.findall(pattern, string, flags=0)\n",
    "Return all matches of pattern in string, as a list of strings.\n",
    "\n",
    "re.sub(pattern, repl, string, count=0, flags=0)\n",
    "Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl.\n",
    "\n",
    "\n",
    "re.search(pattern, string, flags=0)\n",
    "Scan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding match object.\n",
    "\n",
    "re.match(pattern, string, flags=0)\n",
    "If zero or more characters at the beginning of string match the regular expression pattern, return a corresponding match object.\n",
    "re.match() will only match at the beginning of the string\n",
    "\n",
    "\n",
    "##  Metacharacters\n",
    "---> . (dot): ANY ONE character except newline. Same as [^\\n]\n",
    "\n",
    "---> \\d, \\D: ANY ONE digit/non-digit character. Digits are [0-9]\n",
    "\n",
    "---> \\w, \\W: ANY ONE word/non-word character. For ASCII, word characters are [a-zA-Z0-9_]\n",
    "\n",
    "---> \\s, \\S: ANY ONE space/non-space character. For ASCII, whitespace characters are [ \\n\\r\\t\\f]\n",
    "\n",
    "---> ^, $: start-of-line and end-of-line respectively. E.g., ^[0-9]$ matches a numeric string.\n",
    "\n",
    "---> \\b: boundary of word, i.e., start-of-word or end-of-word. E.g., \\bcat\\b matches the word \"cat\" in the input string.\n",
    "\n",
    "---> \\B: Inverse of \\b, i.e., non-start-of-word or non-end-of-word.\n",
    "\n",
    "###  Occurrence Indicators (or Repetition Operators):\n",
    "---> +: one or more (1+), e.g., [0-9]+ matches one or more digits such as '123', '000'.\n",
    "\n",
    "--->  \\*: zero or more (0+), e.g., [0-9]|* matches zero or more digits. It accepts all those in [0-9]+ plus the empty string.\n",
    "\n",
    "---> ?: zero or one (optional), e.g., [+-]? matches an optional \"+\", \"-\", or an empty string.\n",
    "\n",
    "---> {m,n}: m to n (both inclusive)\n",
    "\n",
    "---> {m}: exactly m times\n",
    "\n",
    "---> {m,}: m or more (m+)\n",
    "\n",
    "\n",
    "### Non Greedy\n",
    "Laziness (Curb Greediness for Repetition Operators): *?, +?, ??, {m,n}?, {m,}?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:  common words that are not interesting for the task at hand. These usually include articles such as ‘a’ and ‘the’, pronouns such as ‘i’ and ‘they’, and prepositions such as ‘to’ and ‘from’, …\n",
      "******\n",
      "clean text:  common words that are not interesting for the task at hand these usually include articles such as a and the pronouns such as i and they and prepositions such as to and from \n"
     ]
    }
   ],
   "source": [
    "def text_cleaning(text):\n",
    "    \"\"\" \n",
    "    Removing Punctuations,digits, and stopwords ;lowercasing;tokenization \n",
    "    input: single text \n",
    "    output: cleaned text\n",
    "    \"\"\"\n",
    "    #making sure the text is string type \n",
    "    text = str(text)\n",
    "    \n",
    "    #lowercasing text\n",
    "    text=text.lower()\n",
    "    #removing the url\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    # removing punctuation marks except . it is used for sentence spearator\n",
    "    text=re.sub(r'[!\\\"#$%&\\'()*+,-/:;<=>?@[\\]^_`{|}~]',\"\",text)  # in \\' and \\\" , blackslash is used for escape chaaracters\n",
    "    # 2nd way: simple way to remove all special symbols\n",
    "    text= re.sub(r'[^a-z0-9\\s\\.]+','',text)   #  since lowercasing earlier,not needed r'[^a-zA-Z0-9\\s]+' \n",
    "    \n",
    "    # removing newlines \n",
    "    text=re.sub(r'\\n+',\"\",tet)\n",
    "    \n",
    "    tokens=nltk.word_tokenize(text)\n",
    "    text=\" \".join([word for word in tokens if word not in stop_words])\n",
    "    \n",
    "    return text    \n",
    " \n",
    "text=\"common words that are not interesting for the task at hand. These usually include articles such as ‘a’ and ‘the’, pronouns such as ‘i’ and ‘they’, and prepositions such as ‘to’ and ‘from’, …\"\n",
    "clean_text=text_cleaning(text)\n",
    "print(\"original text: \",text)\n",
    "print(\"******\")\n",
    "print(\"clean text: \",clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "\n",
    "!pip install spacy \n",
    "\n",
    "python -m spacy download en_core_web_sm/en_core_web_lg\n",
    "\n",
    "spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It has inbuild pipeline for nlp operations such as tokenization,POS tagging, Dependency Parsing,lemmatization,named entity recognization etc\n",
    "\n",
    "text : text\n",
    "\n",
    "pos_ : pos tagg\n",
    "\n",
    "dep_ : dependency label\n",
    "\n",
    "lemma_ : lemmatization\n",
    "\n",
    "ents:  names entities\n",
    "\n",
    "sents : sentences \n",
    "\n",
    " ? dependency parser\n",
    " \n",
    "\n",
    "## Pipeline\n",
    "When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the trained pipelines typically include a tagger, a lemmatizer, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
    "\n",
    "pipline components :['parser','tagger','ner','lemmatizer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "#pipeline = [\"tok2vec\", \"tagger\", \"parser\", \"ner\"]\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "******\n",
      "is be AUX VBZ aux xx True True\n",
      "******\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "******\n",
      "at at ADP IN prep xx True True\n",
      "******\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "******\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "******\n",
      "startup startup NOUN NN advcl xxxx True False\n",
      "******\n",
      "for for ADP IN prep xxx True True\n",
      "******\n",
      "$ $ SYM $ quantmod $ False False\n",
      "******\n",
      "1 1 NUM CD compound d False False\n",
      "******\n",
      "billion billion NUM CD pobj xxxx True False\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "## tokenization using spacy by default\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "    print(\"******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apple is looking at buying U.K. startup for $1 billion., The module defines several functions, constants, and an exception.]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# using spacy using sentencizer pipeline\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def cleaning_sentences(text):\n",
    "    doc=nlp(text)   # text having multiple sentences ending with . (dot)\n",
    "    sentences=[sent for sent in doc.sents]   # break down into list of sentences \n",
    "    \n",
    "    return sentences\n",
    "    \n",
    "    \n",
    "text =\"Apple is looking at buying U.K. startup for $1 billion. The module defines several functions, constants, and an exception.\"\n",
    "\n",
    "print(cleaning_sentences(text))\n",
    "print(len(cleaning_sentences(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dependency Parsing   ?\n",
    "\n",
    "Define the dependency labels for words in text\n",
    "\n",
    "we can plot dependency tree using nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mkumari9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "pos_tagged_sent = nltk.pos_tag(sentence.split())\n",
    "print(pos_tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT brown/JJ fox/NN)\n",
      "  (VP is/VBZ)\n",
      "  (ADJP quick/JJ)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  (VP is/VBZ jumping/VBG)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grammar = '''\n",
    "            NP: {<DT>?<JJ>?<NN.*>}  \n",
    "            ADJP: {<JJ>}\n",
    "            ADVP: {<RB.*>}\n",
    "            PP: {<IN>}      \n",
    "            VP: {<MD>?<VB.*>+}\n",
    "          '''\n",
    "\n",
    "\n",
    "rp = nltk.RegexpParser(grammar)\n",
    "shallow_parsed_sent = rp.parse(pos_tagged_sent)\n",
    "print(shallow_parsed_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling contractions\n",
    "\n",
    "Contractions are words that are shortened by dropping letters and replacing them by an apostrophe.\n",
    "example:I’ll, wed'nt etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define examples of contractions\n",
    "\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey I am Yann, how are you and how is it going ? That is interesting: I would love to hear more about it.\n"
     ]
    }
   ],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "test = \"Hey I'm Yann, how're you and how's it going ? That's interesting: I'd love to hear more about it.\"\n",
    "print(decontracted(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the frequent words ,rare words ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'of': 3, 'the': 2, 'The': 1, 'study': 1, 'mechanical': 1, 'or': 1, '\"formal\"': 1, 'reasoning': 1, 'began': 1, 'with': 1, 'philosophers': 1, 'and': 1, 'mathematicians': 1, 'in': 1, 'antiquity.': 1, 'They': 1, 'failed': 1, 'to': 1, 'recognize': 1, 'difficulty': 1, 'some': 1, 'remaining': 1, 'tasks': 1}) \n",
      "\n",
      "most occurring words:  [('of', 3), ('the', 2), ('The', 1), ('study', 1), ('mechanical', 1)] \n",
      "\n",
      "least occurring words:  ['study', 'or', 'to', 'some', 'and', 'in', 'antiquity.', 'with', 'difficulty', 'The', 'philosophers', 'They', 'the', 'of', '\"formal\"', 'mechanical', 'reasoning', 'began', 'recognize', 'remaining', 'mathematicians', 'failed', 'tasks']\n",
      "rare words:  [('tasks', 1), ('remaining', 1), ('some', 1), ('difficulty', 1)]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter   # how counter works\n",
    "\n",
    "n_rare_words=4\n",
    "# frequent words\n",
    "docs= ['The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity.','They failed to recognize the difficulty of some of the remaining tasks']\n",
    "cnt = Counter()\n",
    "for text in docs:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "\n",
    "print(cnt,\"\\n\")\n",
    "print(\"most occurring words: \",cnt.most_common(5),\"\\n\")\n",
    "\n",
    "least_occurring= sorted(list(cnt),key=lambda x: x[1], reverse=True)  # sorting by second item in list of tuples  ? not good result\n",
    "print(\"least occurring words: \",least_occurring)\n",
    "\n",
    "rare_words=cnt.most_common()[:-n_rare_words-1:-1]    #?  # print list elements in reverse order\n",
    "\n",
    "print(\"rare words: \", rare_words)\n",
    "least_ocurring2=list(cnt).sort(key=lambda x:x[1], reverse=True)\n",
    "print(least_ocurring2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philosophers and mathematicians in antiquity.\n"
     ]
    }
   ],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "print(remove_freqwords(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They failed to recognize the difficulty of of the\n"
     ]
    }
   ],
   "source": [
    "n_rare_words = 3\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "print(remove_rarewords(docs[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch01%20-%20Natural%20Language%20Processing%20Basics/Ch01%20-%20Natural%20Language%20Processing%20Basics.ipynb\n",
    "\n",
    "https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
